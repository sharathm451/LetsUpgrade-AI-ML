{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACKPROPAGATION AND FEEDFORWARD OF NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMPLE NEURAL NETWORK METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n"
     ]
    }
   ],
   "source": [
    "# x = no. of hours sleeping, no. of hours studing) , y = test score of the student on x \n",
    "X = np.array(([2,9],[1,5],[3,6]), dtype = float)\n",
    "y = np.array(([92],[86],[89]), dtype = float) \n",
    "\n",
    "# scale units\n",
    "X = X/np.max(X,axis=0) # maximum of x array\n",
    "y = y/100  # maximum of test score is 100\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "actual output: [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "loss:  0.00012465496234554196\n",
      "\n",
      "\n",
      "predicted output:  [[0.91071503]\n",
      " [0.8544489 ]\n",
      " [0.90602933]]\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self):\n",
    "        self.inputsize = 2\n",
    "        self.outputsize = 1\n",
    "        self.hiddensize = 3\n",
    "        \n",
    "        # heights \n",
    "        self.w1 = np.random.randn(self.inputsize, self.hiddensize) # (3*2) wiights from input layer to hidden layer\n",
    "        self.w2 = np.random.randn(self.hiddensize, self.outputsize) # (3*1) weights from hidden layer to output layer\n",
    "        \n",
    "    def forwardfeed(self, X): \n",
    "        self.z = np.dot(X,self.w1) # the product of inputs x and the first set of weights(3*2) \n",
    "        self.z2 = self.sigmoid(self.z) # its activation function where normalize the variable 0 to 1\n",
    "        self.z3 = np.dot(self.z2, self.w2) # the product of hidden layer (z2) and second set of weights(3*1)\n",
    "        output = self.sigmoid(self.z3)\n",
    "        return output\n",
    "        \n",
    "    def sigmoid(self, s, deriv = False): # sigmoid derivative is function used in backpropagation to find the tangent\n",
    "        if  ( deriv == True):           # of the slope. so where actually minimize the cost or loss of function by finding \n",
    "            return s * (1 - s)          # the optimal set of  model parameters. \n",
    "        return 1/ (1 + np.exp(-s))      # if its forwardfeed where sigmoid deriv has to turn off\n",
    "    \n",
    "    def backpropagate(self, X, y, output):\n",
    "        self.output_error = y - output # error in output\n",
    "        self.output_delta = self.output_error * self.sigmoid(output, deriv=True) # applying sigmoid derivative to output error\n",
    "        \n",
    "        self.z2_error = self.output_delta.dot(self.w2.T) #z2 error: how much our hidden layer weigts contributes to output error \n",
    "        self.z2_delta = self.z2_error * self.sigmoid(self.z2, deriv= True)# applying sigmoid derivative to z2 error\n",
    "        \n",
    "        self.w1 += X.T.dot(self.z2_delta)  # its high time to update our weights in input to hidden layer\n",
    "        self.w2 += self.z2.T.dot(self.output_delta)# updating weights in hidden layer to output layer\n",
    "\n",
    "    def train(self, X, y):\n",
    "        output = self.forwardfeed(X) \n",
    "        self.backpropagate(X, y, output) \n",
    "        \n",
    "NN = NeuralNetwork()\n",
    "\n",
    "for i in range(1000): # no. of times to train\n",
    "    NN.train(X, y) \n",
    "\n",
    "print(\"input:\", (X))\n",
    "print(\"actual output:\", (y))\n",
    "print(\"loss: \", (np.mean(np.square(y - NN.forwardfeed(X)))))\n",
    "print(\"\\n\")\n",
    "print(\"predicted output: \", (NN.forwardfeed(X)))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights: \n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "Synaptic weights after training: \n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n",
      "Output After Training:\n",
      "[[0.00966449]\n",
      " [0.99211957]\n",
      " [0.99358898]\n",
      " [0.00786506]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function to normalize inputs\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# sigmoid derivatives to adjust synaptic weights\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# input dataset\n",
    "training_inputs = np.array([[0,0,1],\n",
    "                            [1,1,1],\n",
    "                            [1,0,1],\n",
    "                            [0,1,1]])\n",
    "\n",
    "# output dataset\n",
    "training_outputs = np.array([[0,1,1,0]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0 to create weight matrix, synaptic weights\n",
    "synaptic_weights = 2 * np.random.random((3,1)) - 1\n",
    "\n",
    "print('Random starting synaptic weights: ')\n",
    "print(synaptic_weights)\n",
    "\n",
    "# Iterate 10,000 times\n",
    "for iteration in range(10000):\n",
    "\n",
    "    # Define input layer\n",
    "    input_layer = training_inputs\n",
    "    # Normalize the product of the input layer with the synaptic weights\n",
    "    outputs = sigmoid(np.dot(input_layer, synaptic_weights))\n",
    "\n",
    "    # how much did we miss?\n",
    "    error = training_outputs - outputs\n",
    "\n",
    "    # multiply how much we missed by the\n",
    "    # slope of the sigmoid at the values in outputs\n",
    "    adjustments = error * sigmoid_derivative(outputs)\n",
    "\n",
    "    # update weights\n",
    "    synaptic_weights += np.dot(input_layer.T, adjustments)\n",
    "\n",
    "print('Synaptic weights after training: ')\n",
    "print(synaptic_weights)\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### complete process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights: \n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "Synaptic weights after training: \n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n",
      "Input 1: 1\n",
      "Input 2: 0\n",
      "Input 3: 0\n",
      "New situation: input data =  1 0 0\n",
      "Output data: \n",
      "[0.99993704]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Seed the random number generator\n",
    "        np.random.seed(1)\n",
    "\n",
    "        # Set synaptic weights to a 3x1 matrix,\n",
    "        # with values from -1 to 1 and mean 0\n",
    "        self.synaptic_weights = 2 * np.random.random((3, 1)) - 1\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Takes in weighted sum of the inputs and normalizes\n",
    "        them through between 0 and 1 through a sigmoid function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"\n",
    "        The derivative of the sigmoid function used to\n",
    "        calculate necessary weight adjustments\n",
    "        \"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def train(self, training_inputs, training_outputs, training_iterations):\n",
    "        \"\"\"\n",
    "        We train the model through trial and error, adjusting the\n",
    "        synaptic weights each time to get a better result\n",
    "        \"\"\"\n",
    "        for iteration in range(training_iterations):\n",
    "            # Pass training set through the neural network\n",
    "            output = self.think(training_inputs)\n",
    "\n",
    "            # Calculate the error rate\n",
    "            error = training_outputs - output\n",
    "\n",
    "            # Multiply error by input and gradient of the sigmoid function\n",
    "            # Less confident weights are adjusted more through the nature of the function\n",
    "            adjustments = np.dot(training_inputs.T, error * self.sigmoid_derivative(output))\n",
    "\n",
    "            # Adjust synaptic weights\n",
    "            self.synaptic_weights += adjustments\n",
    "\n",
    "    def think(self, inputs):\n",
    "        \"\"\"\n",
    "        Pass inputs through the neural network to get output\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs = inputs.astype(float)\n",
    "        output = self.sigmoid(np.dot(inputs, self.synaptic_weights))\n",
    "        return output\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize the single neuron neural network\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "    print(\"Random starting synaptic weights: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    # The training set, with 4 examples consisting of 3\n",
    "    # input values and 1 output value\n",
    "    training_inputs = np.array([[0,0,1],\n",
    "                                [1,1,1],\n",
    "                                [1,0,1],\n",
    "                                [0,1,1]])\n",
    "\n",
    "    training_outputs = np.array([[0,1,1,0]]).T\n",
    "\n",
    "    # Train the neural network\n",
    "    neural_network.train(training_inputs, training_outputs, 10000)\n",
    "\n",
    "    print(\"Synaptic weights after training: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    A = str(input(\"Input 1: \"))\n",
    "    B = str(input(\"Input 2: \"))\n",
    "    C = str(input(\"Input 3: \"))\n",
    "    \n",
    "    print(\"New situation: input data = \", A, B, C)\n",
    "    print(\"Output data: \")\n",
    "    print(neural_network.think(np.array([A, B, C])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
